{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!USAR PIP EN LA CONSOLA DE ANACONDAAAAAAAAAAAAAAAAAAA!!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "from patchify import patchify, unpatchify\n",
    "import tifffile as tiff\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "a1D = np.array([[1, 2, 3, 4],[1, 2, 3, 4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleMinMax(x):\n",
    "    # print(np.nanmin(x))\n",
    "    # print(np.nanmax(x))\n",
    "    return ((x-np.nanmin(x))/(np.nanmax(x)-np.nanmin(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "# get the path/directory\n",
    "large_image_stack =[]\n",
    "large_mask_stack =[]\n",
    "all_complete_images=[]\n",
    "folder_dir = 'C:/Users/lucas/Desktop/ai/MitocondraAndFireSCARSAI/U-net-model-FIRE/AIscarsImages'\n",
    "stop = 0\n",
    "for images in os.listdir(folder_dir):\n",
    "    # print(images)\n",
    "    img = tiff.imread(\n",
    "        'C:/Users/lucas/Desktop/ai/MitocondraAndFireSCARSAI/U-net-model-FIRE/AIscarsImages/'+images)\n",
    "    redArray =img[:,:,4]\n",
    "    greenArray = img[:,:,3]\n",
    "    blueArray = img[:,:,2]\n",
    "    redArray = scaleMinMax(redArray)\n",
    "    greenArray = scaleMinMax(greenArray)\n",
    "    blueArray = scaleMinMax(blueArray)\n",
    "    # im = Image.fromarray()\n",
    "\n",
    "    large_image_stack.append(np.uint8(np.array([redArray,greenArray,blueArray]).transpose(1,2,0)*255))\n",
    "\n",
    "    large_mask_stack.append(np.uint8(np.array([img[:,:,7]]).transpose(1,2,0)) )\n",
    "    all_complete_images.append(img)\n",
    "    stop = stop+1\n",
    "    # if stop > 30:\n",
    "    #     breaktenes \n",
    "large_image_stack[0].shape,len(large_image_stack),large_mask_stack[0].shape,len(large_mask_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = np.array(patchesImages)\n",
    "# images = np.expand_dims(images, -1)\n",
    "# masks = np.array(patchesMask)\n",
    "images = (large_image_stack)\n",
    "# masks = np.expand_dims(masks, -1) \n",
    "# images = np.expand_dims(images, -1)\n",
    "masks = (large_mask_stack)\n",
    "# masks = np.expand_dims(masks, -1) \n",
    "# images.shape,masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "image_number_1 = random.randint(0, 30)\n",
    "image_number_2 = random.randint(0, 30)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x = plt.subplot(121)\n",
    "plt.imshow(images[image_number_1])\n",
    "plt.subplot(122)\n",
    "plt.imshow(masks[image_number_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchesImages = []\n",
    "patchesMask= []\n",
    "imageSize = 512\n",
    "for img in range(len(images)):\n",
    "    #print(img)     #just stop here to see all file names printed\n",
    "    large_image = images[img]\n",
    "    patch = patchify(large_image, (imageSize, imageSize,3), step=imageSize)  #Step=imageSize for imageSize patches means no overlap\n",
    "    # print(patch.shape)\n",
    "    patch = np.squeeze(patch)\n",
    "    # print(patch.shape)\n",
    "\n",
    "    for i in range(patch.shape[0]):\n",
    "        for j in range(patch.shape[1]):\n",
    "            \n",
    "            single_patch_image = patch[i,j,:,:,:]\n",
    "            patchesImages.append(single_patch_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for img in range(len(masks)):\n",
    "    #print(img)     #just stop here to see all file names printed\n",
    "    large_image = masks[img]\n",
    "    patch = patchify(large_image, (imageSize, imageSize,1), step=imageSize)  #Step=imageSize for imageSize patches means no overlap\n",
    "    # print(patch.shape)\n",
    "    patch = np.squeeze(patch)\n",
    "    # print(patch.shape)\n",
    "\n",
    "    for i in range(patch.shape[0]):\n",
    "        for j in range(patch.shape[1]):\n",
    "            \n",
    "            single_patch_image = patch[i,j,:,:]\n",
    "            # single_patch_image = single_patch_image / imageSize-1\n",
    "            patchesMask.append(single_patch_image)\n",
    "\n",
    "len(patchesImages),len(patchesMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(patchesImages)\n",
    "# images = np.expand_dims(images, -1)\n",
    "masks = np.array(patchesMask)\n",
    "# masks = np.expand_dims(masks, -1) \n",
    "images.shape,masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "image_number_1 = random.randint(0, 72)\n",
    "image_number_2 = random.randint(0, 72)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(images[image_number_1])\n",
    "plt.subplot(122)\n",
    "plt.imshow(masks[image_number_1])\n",
    "# images[image_number_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len(images),images[0].shape,len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mini_batch(imgs,masks):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for i in range(0,32):\n",
    "        #subplot recibe el número de filas, num de cols, y pos\n",
    "        #!ojo subplot arranca de 1, NO de 0\n",
    "        plt.subplot(4,8,i+1)\n",
    "        img = imgs[i]\n",
    "        mask = masks[i]\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(mask,alpha=0.3)\n",
    "        plt.axis('Off')\n",
    "        # print(img.shape)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mini_batch(images,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mini_batch(imgs,masks):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for i in range(0,32):\n",
    "        #subplot recibe el número de filas, num de cols, y pos\n",
    "        #!ojo subplot arranca de 1, NO de 0\n",
    "        plt.subplot(4,8,i+1)\n",
    "        img = imgs[i]\n",
    "        mask = masks[i]\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(mask,alpha=0.3)\n",
    "        plt.axis('Off')\n",
    "        # print(img.shape)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mini_batch(images,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construyendo una imagen  pil\n",
    "\n",
    "from PIL import Image\n",
    "# from matplotlib import cm\n",
    "\n",
    "# large_image_stack.append(np.array([redArray,greenArray,blueArray]).transpose(1,2,0))\n",
    "\n",
    "# im = Image.fromarray(large_mask_stack)\n",
    "# print(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newShapeImages= images.transpose(0,3,1,2)\n",
    "# for i in newShapeImages:\n",
    "#     for a in i:\n",
    "#         for s in range(0,a.shape[0]):\n",
    "#             for t in range(0,a.shape[1]):\n",
    "#                 if(np.isnan(a[s,t])):\n",
    "#                     print(\"el numero %d\",i,\"es nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(images.shape, masks.shape)\n",
    "# for index,i in enumerate(masks):\n",
    "#     print(np.nanmax(i),index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newShapeImages = masks\n",
    "\n",
    "# for  index,i  in enumerate(newShapeImages):\n",
    "#         for s in range(0,i.shape[0]):\n",
    "#             for t in range(0,i.shape[1]):\n",
    "#                 if(np.isnan(i[s,t])):\n",
    "#                     print(\"el masks %d\",index,\"es nan\")\n",
    "# len(newShapeImages)\n",
    "#!PARECE HABER ERRORES EN EL ITERADOR\n",
    "# newShapeImages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_images_and_masks():\n",
    "\n",
    "    return images,masks\n",
    "\n",
    "class Satelite_Dataset(Dataset):\n",
    "    def __init__(self, data, img_transforms=None, mask_transforms=None):\n",
    "        #\n",
    "        # data -train data path\n",
    "        # mask -trans masks path\n",
    "        #\n",
    "        self.train_data = data\n",
    "\n",
    "        self.img_transforms = img_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "\n",
    "        self.images,self.masks = return_images_and_masks()\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.images is not None:\n",
    "            assert len(self.images) == len(self.masks), 'not the same number of imaegs and masks'\n",
    "        return len(self.images)\n",
    "\n",
    "    # nos devuelve un elemento del data set en la posición index\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = Image.fromarray(self.images[idx])\n",
    "        mask = np.array(self.masks[idx])\n",
    "        trans = T.ToTensor()\n",
    "        if self.img_transforms is not None:\n",
    "            img = self.img_transforms(img)\n",
    "            # print(\"imagen transformada\")\n",
    "            # print(\"img\",img)\n",
    "        else:\n",
    "            img = trans(img)\n",
    "        if self.img_transforms is not None:\n",
    "            mask = self.img_transforms(mask)\n",
    "        else:\n",
    "            mask= trans(img)\n",
    "                \n",
    "        mask_max = mask.max().item() +  1e-8\n",
    "        mask /= mask_max\n",
    "        return img, mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = T.Compose([\n",
    "    # T.Resize([512,512]),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "TRAIN_PATH ='C:/Users/lucas/Desktop/ai/MitocondraAndFireSCARSAI/ImagesFromDrive/AIscarsImages/'\n",
    "full_dataset  = Satelite_Dataset(\n",
    "    TRAIN_PATH,#actualmente no hace nada pero se puede hacer\n",
    "    img_transforms=transform_data,\n",
    "    mask_transforms=transform_data)\n",
    "# type(full_dataset.__getitem__(3)),full_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#datos de validación    \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SIZE = int(len(full_dataset)*0.8)\n",
    "VAL_SIZE = len(full_dataset)-TRAIN_SIZE\n",
    "TRAIN_SIZE,VAL_SIZE\n",
    "train_dataset, val_dataset = random_split(full_dataset, [TRAIN_SIZE, VAL_SIZE])\n",
    "print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  \n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks = next(iter(train_loader))\n",
    "print(imgs.shape, masks.shape)\n",
    "for index,i in enumerate(masks):\n",
    "    print(np.nanmax(i),index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks = next(iter(train_loader))\n",
    "def plot_mini_batch(imgs,masks):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        #subplot recibe el número de filas, num de cols, y pos\n",
    "        #!ojo subplot arranca de 1, NO de 0\n",
    "        plt.subplot(4,8,i+1)\n",
    "        img = imgs[i,...].permute(1,2,0).numpy()\n",
    "        mask = masks[i,...].permute(1,2,0).numpy()\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(mask,alpha=0.8)\n",
    "        plt.axis('Off')\n",
    "        # print(img.shape)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mini_batch(imgs,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Double_Conv(nn.Module):\n",
    "    ''' \n",
    "    Es la doble convolución que se hace luego de cada bajada\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            Conv_3_K(channels_in, channels_out),\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "            nn.ReLU(),  # ? Ponemos la relu para cortar con la linealidad\n",
    "            Conv_3_K(channels_out, channels_out),\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down_Conv(nn.Module):\n",
    "    ''' \n",
    "    Es la doble convolución que se hace luego de cada bajada\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.MaxPool2d(2,2),\n",
    "            Double_Conv(channels_in,channels_out)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Conv_3_K(nn.Module):\n",
    "    ''' \n",
    "     Simplemente definimos la conv y como para todas las capas son las mismas no pasa nada\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv1(x)\n",
    "\n",
    "class Up_conv(nn.Module):\n",
    "    ''' \n",
    "     Simplemente definimos la conv y como para todas las capas son las mismas no pasa nada\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.upsample_layer = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2,mode='bicubic'),\n",
    "            nn.Conv2d(channels_in,channels_in//2,1,stride=1),\n",
    "\n",
    "        )\n",
    "        self.decoder = Double_Conv(channels_in,channels_out)\n",
    "    def forward(self, x1,x2):\n",
    "        #en un minibatch de pytorch la primera nos dice la dimension el numero de elementos del mini batch, el sdo los canales, ydsps ancho y alto\n",
    "        x_upsample = self.upsample_layer(x1)\n",
    "        #buscamos entonces alinear los canales de los 3\n",
    "        x = torch.cat([x2,x_upsample],dim=1)\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ME marea que por ej  cuando declaras la función nn.conv2d no declaras los parametros de entrada si nomas bien la configuración\n",
    "#acordarse que al crear la clase solamente creas, y no eejcutas el forward, el forward se ejecuta cuando a donde asignaste la clase, en e lcaso de abajo a model, le pones parentesis, asi model()\n",
    "class UNET (nn.Module):\n",
    "    # chanes in son los primeros canales, chanels es un canal base? raro\n",
    "    def __init__(self, channels_in, channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.first_conv = Double_Conv(channels_in, channels)\n",
    "        # recibe ya una mezcla con 64 canales, y ya maxpooleada toda la mezcla\n",
    "        # luego entra al down_conv\n",
    "        self.down_1 = Down_Conv(channels, channels*2)  # sale con 128\n",
    "        self.down_2 = Down_Conv(channels*2, channels*4)  # sale con 256\n",
    "        self.down_3 = Down_Conv(channels*4, channels*8)  # sale con 512\n",
    "\n",
    "        self.middle_conv = Down_Conv(channels*8, channels*16)  # sale con 1024\n",
    "\n",
    "        self.up_1 = Up_conv(channels*16, channels*8)\n",
    "        self.up_2 = Up_conv(channels*8, channels*4)\n",
    "        self.up_3 = Up_conv(channels*4, channels*2)\n",
    "        self.up_4 = Up_conv(channels*2, channels) \n",
    "\n",
    "        self.last_conv = nn.Conv2d(channels, num_classes, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_conv_1 = self.first_conv(x)\n",
    "        x_conv_2 = self.down_1(x_conv_1) \n",
    "        x_conv_3 = self.down_2(x_conv_2)\n",
    "        x_conv_4 = self.down_3(x_conv_3)\n",
    "\n",
    "        middle_conv = self.middle_conv(x_conv_4)\n",
    "\n",
    "        x_up_1 = self.up_1(middle_conv,x_conv_4)\n",
    "        x_up_2 = self.up_2(x_up_1,x_conv_3)\n",
    "        x_up_3 = self.up_3(x_up_2,x_conv_2)\n",
    "        x_up_4 = self.up_4(x_up_3,x_conv_1)\n",
    "        \n",
    "        return self.last_conv(x_up_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    correct = 0\n",
    "    intersection = 0\n",
    "    denom = 0\n",
    "    union = 0\n",
    "    total = 0\n",
    "    cost = 0.\n",
    "    model = model.to(device=device)\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype = torch.float32)\n",
    "            y = y.to(device=device, dtype = torch.long).squeeze(1)\n",
    "            scores = model(x)\n",
    "            cost += (F.cross_entropy(scores, y)).item()\n",
    "            # standard accuracy not optimal\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            correct += (preds == y).sum()\n",
    "            total += torch.numel(preds)\n",
    "            #dice coefficient\n",
    "            intersection += (preds*y).sum()\n",
    "            denom += (preds + y).sum()\n",
    "            dice = 2*intersection/(denom + 1e-8)\n",
    "            #intersection over union\n",
    "            union += (preds + y - preds*y).sum()\n",
    "            iou = (intersection)/(union + 1e-8)\n",
    "            \n",
    "        return cost/len(loader), float(correct)/total, dice, iou    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, optimiser, start_val = 1e-6, end_val = 1, beta = 0.99, loader = train_loader):\n",
    "    n = len(loader) - 1\n",
    "    factor = (end_val / start_val)**(1/n)\n",
    "    lr = start_val\n",
    "    optimiser.param_groups[0]['lr'] = lr #this allows you to update the learning rate\n",
    "    avg_loss, loss, acc = 0., 0., 0.\n",
    "    lowest_loss = 0.\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    accuracies = []\n",
    "    model = model.to(device=device)\n",
    "    for i, (x, y) in enumerate(loader, start=1):\n",
    "        x = x.to(device =device, dtype = torch.float32)\n",
    "        y = y.to(device =device, dtype = torch.long).squeeze(1)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        scores = model(x)\n",
    "        cost = F.cross_entropy(input=scores, target=y)\n",
    "        loss = beta*loss + (1-beta)*cost.item()\n",
    "        #bias correction\n",
    "        avg_loss = loss/(1 - beta**i)\n",
    "        \n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        acc_ = (preds == y).sum()/torch.numel(scores)\n",
    "#         acc = beta*acc + (1-beta)*acc_.item()\n",
    "#         avg_acc = acc/(1 - beta**i)\n",
    "        #if loss is massive stop\n",
    "        if i > 1 and avg_loss > 4 * lowest_loss:\n",
    "            print(f'from here{i, cost.item()}')\n",
    "            return log_lrs, losses, accuracies\n",
    "        if avg_loss < lowest_loss or i == 1:\n",
    "            lowest_loss = avg_loss\n",
    "\n",
    "        accuracies.append(acc_.item())\n",
    "#         accuracies.append(avg_acc)\n",
    "        losses.append(avg_loss)\n",
    "        log_lrs.append(lr)\n",
    "        #step\n",
    "        cost.backward()\n",
    "        optimiser.step()\n",
    "        #update lr\n",
    "        print(f'cost:{cost.item():.4f}, lr: {lr:.4f}, acc: {acc_.item():.4f}')\n",
    "        lr *= factor\n",
    "        optimiser.param_groups[0]['lr'] = lr\n",
    "        \n",
    "    return log_lrs, losses, accuracies     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimiser,scheduler=None, epochs=100, store_every = 25):\n",
    "    model = model.to(device=device)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch number is \",epoch)\n",
    "        train_correct_num = 0\n",
    "        train_total = 0\n",
    "        train_cost_acum = 0.\n",
    "        for mb, (x,y) in enumerate(train_loader,start=1):\n",
    "            model.train()\n",
    "            x = x.to(device=device,dtype=torch.float32)\n",
    "            y = y.to(device=device,dtype=torch.long).squeeze(1) #investigarp orque se saca el canal,minuto 3.31 parte 3\n",
    "            scores = model(x)\n",
    "            cost = F.cross_entropy(input=scores,target=y)\n",
    "            optimiser.zero_grad()\n",
    "            cost.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            if scheduler: scheduler.step()\n",
    "            #recordar que scores devuelve dos canales, entonces lo que me interesa es quedarme con los valore que mas prob tengan en cada pixel\n",
    "            train_predictions = torch.argmax(scores,dim=1) #estos son todos los valores maximos de las dos capas comparadas.\n",
    "            train_correct_num += (train_predictions == y).sum() #esta es la cantidad de valores correctos\n",
    "            train_total += torch.numel(train_predictions) #busca acumular todos los pixeles de cada img\n",
    "            train_cost_acum+=cost.item() #el costo es un tensor de pytorch asique va item()\n",
    "            \n",
    "            if mb%store_every == 0:\n",
    "                val_cost,val_acc,dice,iou= accuracy(model,val_loader)#!!prestar atención aca, le da todo el minibatch para que evalue el accuracy a lo largo?\n",
    "                \n",
    "                train_acc = float(train_correct_num)/train_total #es el total de predicciones correctas sobre el total de pixeles\n",
    "\n",
    "                train_cost_every = float(train_cost_acum)/mb\n",
    "                print(f'mb:{mb}, train cost:{train_cost_every:.4f} val cost :{val_cost:.4f},'\n",
    "                      f'trian acc: {train_acc:.4f} val_acc :{val_acc:.4f},'\n",
    "                      f'dice: {dice},iou:{iou}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42) \n",
    "model = UNET(3, 4, 2)\n",
    "epochs=4\n",
    "optimizer_unet = torch.optim.SGD(model.parameters(),\n",
    "                                 lr=0.01, momentum=0.95,\n",
    "                                 weight_decay=1e-4)\n",
    "scheduler =torch.optim.lr_scheduler.OneCycleLR(optimizer_unet,\n",
    "                                               max_lr=1e-1,\n",
    "                                               steps_per_epoch=len(train_loader),\n",
    "                                               epochs=epochs,\n",
    "                                               pct_start=0.43,\n",
    "                                               div_factor=10,\n",
    "                                               final_div_factor=1000,\n",
    "                                               three_phase=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,optimizer_unet,scheduler,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs_val, masks_val = next(iter(val_loader)) #recordar que los loaders te devuelven un wrapeado de batchs basicamente, entonces podes iterar en cada batc\n",
    "print(imgs_val.shape)\n",
    "imgs_val = imgs_val.to(device, dtype=torch.float32)\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    scores = model(imgs_val)\n",
    "    preds = torch.argmax(scores, dim=1).float()\n",
    "    imgs_val = imgs_val.cpu()\n",
    "    preds = preds.cpu()\n",
    "    print (preds.shape)\n",
    "\n",
    "plot_mini_batch(imgs_val, preds.unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subplot recibe el número de filas, num de cols, y pos\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "image_number_1 = random.randint(0, 31)\n",
    "\n",
    "img = imgs_val[image_number_1].permute(1,2,0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "mask = preds[image_number_1].numpy()\n",
    "plt.imshow(mask,alpha=0.8)\n",
    "plt.axis('Off')\n",
    "# print(img.shape)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
